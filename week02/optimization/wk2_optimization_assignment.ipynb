{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 15기 2주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16168248, 0.06287374, 0.0368519 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "# (n, 1) 사이즈의 벡터 X와 (n, 1) 사이즈의 벡터 parameters의 내적을 구한다.\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i]*parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = {1 \\over1+e^{-X_i\\theta}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    # logistic 함수를 계산한다.\n",
    "    z = np.exp(-dot_product(X, parameters))\n",
    "#     z = np.exp(-np.dot(X, parameters))\n",
    "    p = 1/(1+z)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5591820988778776"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "## $l(p) =-\\Sigma \\begin{bmatrix}y_i \\log p+(1-y_i)\\log(1-p)\\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = -y*np.log(p)+(1-y)*np.log(1-p)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X, parameters)\n",
    "    loss = 0.5*np.sum((y-parameters)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X, y, parameters)\n",
    "    loss = loss/n #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5166539183871012"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)=-\\Sigma (y_i-\\theta^TX_i)X_{j}$\n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)=-\\Sigma (y_i-p_i)x_{ij}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X, parameters)\n",
    "        gradient = -(y-y_hat)*X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = -(y-p)*X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0877848787524356"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "\n",
    "    for i in range(len(y_set)):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model)\n",
    "            # 데이터별로 j번째 factor의 gradinet를 계산하여 합산한다.\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39.036287208801525, -14.727469071108398, 17.15363728890775]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "### 설명: 주어진 배치사이즈대로 전체 데이터를 나눈다. 이 때, output은 각 배치에 들어가는 원 데이터의 인덱스를 가지고 있다. \n",
    "\n",
    "### 예를 들어, idx_list가 [ [1,2,3],[4,5,6]]와 같이 이뤄졌다면 첫번째 미니배치에는 1번, 2번, 3번 데이터가 들어가고, 두번째 미니배치에는 4,5,6번 데이터가 들어간다는 뜻이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        # 각 데이터별 gradient의 평균에다가 learning rate를 곱해준다.\n",
    "        gradients[i] *= learning_rate/n\n",
    "    \n",
    "    parameters -= gradients # loss가 작아지는 방향으로 파라미터를 업데이트한다.\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15908006, 0.06385557, 0.03570832])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: \n",
    "- num_epoch:\n",
    "<br>\n",
    "\n",
    "BGD: 전체 데이터를 모두 검토하여 기울기를 계산한 후 갱신한다.\n",
    "SGD: 랜덤하게 데이터를 하나 선정하여 기울기를 계산한 후 갱신한다.\n",
    "MGD: 데이터를 미니배치로 쪼개서 미니배치를 랜덤으로 선택하여 기울기를 계산한 후 갱신한다.\n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
    "batch_size=1 -> SGD  \n",
    "batch_size=k -> MGD\n",
    "batch_size=whole -> BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)  # 미니배치 그래디언트를 구한다.\n",
    "            parameters = step(parameters, gradients, learning_rate, batch_size) # 앞에서 구한 그래디언트를 토대로 업데이트한다.\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, batch_size) # 새로운 loss를 계산한다.\n",
    "#             print(new_loss)\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: -0.3515238057397422  params: [0.00275966 0.07038807 0.87735573]  gradients: [0.01099535597968274, 0.001857564518555483, 0.014712470065032155]\n",
      "epoch: 100  loss: -0.01887809423958329  params: [-0.64202969  0.2544271  -0.07192085]  gradients: [0.003349930042397789, -0.003746848994735771, 0.006041960154047421]\n",
      "epoch: 200  loss: 0.0326817130667394  params: [-0.85169862  0.63660301 -0.54750401]  gradients: [0.0013141777636514853, -0.0036409361511229492, 0.003908456665180843]\n",
      "epoch: 300  loss: 0.04010501500760282  params: [-0.94960912  0.96898651 -0.88950928]  gradients: [0.0007586874603854417, -0.0030199151053817356, 0.0030259559883126313]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.95557674,  0.99293887, -0.9134793 ])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, batch_size = len(y_train), learning_rate = 0.05)\n",
    "new_param_bgd\n",
    "# accuracy: 0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: -0.8089059538530856  params: [-0.15386163  0.15605919  0.40994142]  gradients: [0.005571185683727282, 0.0030318445777457043, 0.003919834667861157]\n",
      "epoch: 100  loss: -0.1195497709432269  params: [-1.6259318   3.45731321 -3.2962252 ]  gradients: [0.0011288247519348718, 0.0006143075096878092, 0.0007942306445642932]\n",
      "epoch: 200  loss: -0.09721083123547691  params: [-1.78695827  3.97821394 -3.78041297]  gradients: [0.000927750889194419, 0.0005048829212637218, 0.0006527569363242421]\n",
      "epoch: 300  loss: -0.09082210393119745  params: [-1.84018221  4.14923843 -3.93827692]  gradients: [0.0008694340000169169, 0.0004731468143950817, 0.0006117257130089828]\n",
      "epoch: 400  loss: -0.08856555019786773  params: [-1.85991801  4.21254096 -3.99657553]  gradients: [0.0008487489256185563, 0.0004618899771228803, 0.0005971718861690676]\n",
      "epoch: 500  loss: -0.08771282053262588  params: [-1.86751251  4.2368848  -4.01897628]  gradients: [0.0008409203752237013, 0.00045762967251024154, 0.0005916637905896145]\n",
      "epoch: 600  loss: -0.0873825121088551  params: [-1.87047492  4.24637842 -4.02770933]  gradients: [0.0008378862032601256, 0.000455978473225575, 0.0005895289752870451]\n",
      "epoch: 700  loss: -0.0872533480282274  params: [-1.87163652  4.25010063 -4.0311329 ]  gradients: [0.0008366994522497641, 0.0004553326421906468, 0.0005886939882633453]\n",
      "epoch: 800  loss: -0.0872026529826781  params: [-1.87209291  4.25156305 -4.03247793]  gradients: [0.0008362336286763332, 0.00045507914055644035, 0.000588366239109772]\n",
      "epoch: 900  loss: -0.08718272713367554  params: [-1.87227238  4.2521381  -4.0330068 ]  gradients: [0.0008360505289325752, 0.0004549794975007582, 0.0005882374118252398]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.87234254,  4.25236291, -4.03321356])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, batch_size = 1, learning_rate = 0.01)\n",
    "new_param_sgd\n",
    "# accuracy: 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: -0.38020287753886517  params: [ 0.35865811  0.88664486 -0.13965177]  gradients: [0.05628692408949877, 0.036124843602544476, 0.07052986952877698]\n",
      "epoch: 100  loss: -0.06041446497401403  params: [-1.56782422  3.2711391  -3.12039997]  gradients: [0.007849240263259833, 0.0018515417793601116, 0.0033409808707917763]\n",
      "epoch: 200  loss: -0.08078658199679613  params: [-1.74313054  3.84355375 -3.65378376]  gradients: [0.009975660060160084, 0.0024741457464060278, 0.0010014106656712277]\n",
      "epoch: 300  loss: -0.08848981026808492  params: [-1.81226775  4.0671741  -3.86048294]  gradients: [0.01075759169723184, 0.002696465595769065, 0.0002284689604287668]\n",
      "epoch: 400  loss: -0.0918572547037736  params: [-1.84297959  4.16622347 -3.95175129]  gradients: [0.011094145139669484, 0.0027905937675799237, -9.265110683896147e-05]\n",
      "epoch: 500  loss: -0.0934108868638003  params: [-1.85724994  4.21219268 -3.99405099]  gradients: [0.011248237015560191, 0.0028333355853228694, -0.00023755187428318726]\n",
      "epoch: 600  loss: -0.0941446123416552  params: [-1.86401156  4.23396234 -4.01407011]  gradients: [0.011320739903100148, 0.002853364986620986, -0.0003052865303927258]\n",
      "epoch: 700  loss: -0.0944948578324991  params: [-1.86724427  4.24436777 -4.02363594]  gradients: [0.01135528743440402, 0.0028628902027807816, -0.0003374639814149689]\n",
      "epoch: 800  loss: -0.09466289436721466  params: [-1.86879638  4.2493631  -4.02822754]  gradients: [0.011371847929816473, 0.0028674518081893232, -0.00035286613469310077]\n",
      "epoch: 900  loss: -0.09474370714971148  params: [-1.86954309  4.2517662  -4.03043627]  gradients: [0.011379808953966649, 0.0028696436728781784, -0.00036026520952492635]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.86990023,  4.25291552, -4.03149259])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate= 0.25, batch_size = 32)\n",
    "new_param_mgd\n",
    "# accuracy: 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_mgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38,  2],\n",
       "       [ 1,  9]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "print(\"accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7157328 , 1.99193104])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.43294489022893523  params: [1.13149351 0.63107226]  gradients: [-0.011661887718443573, -0.008784698887387836]\n",
      "epoch: 100  loss: 0.33018520310286237  params: [0.72454951 1.98278142]  gradients: [0.000549052826907534, 0.0005143080998801324]\n",
      "epoch: 200  loss: 0.3312368589408224  params: [0.72262628 1.98639456]  gradients: [0.0005487825432432709, 0.0005217092509951313]\n",
      "epoch: 300  loss: 0.3312399796976779  params: [0.72262059 1.98640524]  gradients: [0.0005487817435803407, 0.0005217311480869029]\n",
      "epoch: 400  loss: 0.33123998893084977  params: [0.72262058 1.98640528]  gradients: [0.0005487817412144568, 0.0005217312128717752]\n",
      "epoch: 500  loss: 0.33123998895816686  params: [0.72262057 1.98640528]  gradients: [0.0005487817412074506, 0.0005217312130634425]\n",
      "epoch: 600  loss: 0.3312399889582463  params: [0.72262057 1.98640528]  gradients: [0.000548781741207427, 0.0005217312130639979]\n",
      "epoch: 700  loss: 0.3312399889582463  params: [0.72262057 1.98640528]  gradients: [0.000548781741207427, 0.0005217312130639979]\n",
      "epoch: 800  loss: 0.3312399889582463  params: [0.72262057 1.98640528]  gradients: [0.000548781741207427, 0.0005217312130639979]\n",
      "epoch: 900  loss: 0.3312399889582463  params: [0.72262057 1.98640528]  gradients: [0.000548781741207427, 0.0005217312130639979]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.72262057, 1.98640528])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y, model = 'linear')\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '^b', label = 'NE', alpha = 0.6) #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
